---
title: "Data Science Homework 5"
author: "ak5357"
date: "2024-11-12"
output: html_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(purrr)

# DEFAULT SETTINGS FOR FIGURE EXPORT
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%",
  fig.align = "center")

theme_set(
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, margin = margin(b = 5), face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, margin = margin(b = 10), color = "azure4", face = "bold", size = 8)))

```

# **Problem 2**

## _Random Normal Samples and T-Tests_

#### **Base scenario (mu = 0)**

Set the design elements.
```{r design_elements}
n = 30
sigma = 5
mu = 0
n_samples = 5000
```

Generate 5000 datasets with random normal distributions with the characteristics described above.
```{r mu_0_samples}
# Create empty list of length 5000
data = vector("list", length = n_samples)

# Generate samples and fill list
for (i in 1:n_samples){
  data[[i]] = rnorm(n, mu, sigma)
}

# Convert list to dataframe for ease of use
data_df =
  tibble(
    id = c(1:n_samples),
    samp = data)
```

For each dataset, save the observed mean and p-value arising from the test below.

$$H_0: \mu = 0 \quad \quad Given: \alpha = 0.05$$

```{r mu_0_t_test}
# Add columns with t_test results, observed mean, and p-value
data_df = data_df |> 
  mutate(
    t_test = map(samp, \(x) broom::tidy(t.test(x))),
    mu_obs = map_dbl(t_test, \(x) x$estimate),
    p_value = map_dbl(t_test, \(x) x$p.value))

# Preview of dataframe
head(data_df, 10)
```

#### **Replicating the process for other scenarios (mu = {1:6})**

Repeat the process above for the following scenarios:

$$\mu = \{ 1, 2, 3, 4, 5, 6 \}$$
To do this I will create two functions which will do the following:

* **Function 1:** Create a dataframe with 5000 samples given the mu value and other parameters (get_norm_samples)
* **Function 2:** Perform the t-test on each sample in the dataframe and save the estimate and p-value from each (perform_t_test)

```{r f_get_norm_samples}
get_norm_samples = function(mu = 0, n = 30, sigma = 5, n_samples = 5000){
  # Conditional
  if (!is.numeric(c(n, sigma, mu, n_samples))){
    stop("all arguments must be numeric")
  }
  
  # Create empty list of length n_samples
  data = vector("list", length = n_samples)
  
  # Generate samples and fill list
  for (i in 1:n_samples){
    data[[i]] = rnorm(n, mu, sigma)
  }
  
  # Convert list to dataframe for ease of use
  data_df =
    tibble(
      id = c(1:n_samples),
      samp = data)
  
  # Return output data_df with all samples
  return(data_df)
}
```

```{r f_perform_t_test}
perform_t_test = function(data_df){
  output_df = data_df |> 
    mutate(
      t_test = map(samp, \(x) broom::tidy(t.test(x))),
      mu_obs = map_dbl(t_test, \(x) x$estimate),
      p_value = map_dbl(t_test, \(x) x$p.value)
    ) |> 
    select(id, mu_obs, p_value)
  
  return(output_df)
}
```

Now that the functions are ready, let's run the process for all mu = {0:6}.
```{r all_mu_processing}
# List of all mu values
mu_list = c(0:6)

# Dataframe containing all mu values and results for each
results_df = 
  tibble(
    mu = mu_list,
    result = map(mu_list, \(x) perform_t_test(get_norm_samples(mu = x)))
  )

# Let's look at the results for mu = 0
results_df$result[[mu == 0]]
```

#### **Visualizing Hypothesis Testing**

Plot the proportion of times the null was rejected for each mu value.
```{r plot_proportion_reject_null}
alpha = 0.05

results_df |> 
  unnest(result) |> 
  mutate(
    reject_h = p_value < alpha) |> 
  group_by(mu) |> 
  summarize(
    pr_reject = mean(reject_h)) |> 
  ggplot(aes(x = mu, y = pr_reject)) +
  geom_line(color = "cornflowerblue", alpha = 0.5) +
  geom_point() +
  labs(
    title = "Null Rejection by Mu Value",
    x = "Mu",
    y = "Proportion of Times Null\nHypothesis was Rejected"
  )
```

#### **Visualizing Mu**

Plot the average estimate of mu compared to the true value of mu, for all samples and for only samples where the null was rejected.
```{r plot_obs_mu}
results_df |> 
  # Manipulate Data ------------------------
  unnest(result) |> 
  #filter(p_value < 0.05) |> 
  group_by(mu) |> 
  summarize(
    avg_mu_obs_RH = mean(mu_obs[p_value < 0.05]),
    avg_mu_obs_ALL = mean(mu_obs)
  ) |>
  pivot_longer(
    cols = -mu,
    names_to = "sample_type",
    values_to = "mu_obs") |> 
  mutate(
    sample_type = case_when(
      sample_type == "avg_mu_obs_RH" ~ "Samples where Null was Rejected",
      sample_type == "avg_mu_obs_ALL" ~ "All Samples")) |>
  # Create Plot ----------------------------
  ggplot(aes(x = mu, y = mu_obs, color = sample_type)) +
  geom_point(alpha = .5, size = 3) +
  geom_line() +
  labs(
    title = "Observed Mu vs. True Mu",
    x = "True Mu",
    y = "Observed Mu",
    color = "Sample Type:"
  )
```

# **Problem 3**

## _Homicide Statistics_

#### **Data Import**

Import Washington Post homicide data.
```{r import_homicide_data, message = FALSE}
homicide_df = read_csv("data/homicide-data.csv") |> 
  mutate(city_state = paste0(city, ", ", state))
```

#### **Describing the Dataset**

```{r describe_homicide_data}
n_cities = homicide_df |> 
  select(city_state) |> 
  n_distinct()

n_states = homicide_df |> 
  select(state) |> 
  n_distinct()
```

**Description:** This dataset contains information about homicide cases in `r n_cities` cities, across `r n_states` states. The following table summarizes some key facts about the ten cities with the highest number of homicide cases.

This dataset was sourced from the Washington Post, which collected data on more than 52,000 criminal homicides over the past decade in the largest American cities. Washington Post reporters received data in many formats and worked for months to clean and standardize it. The resulting dataset includes information on such as location of killing, arrest status, and basic demographics.

```{r summarize_homicide_data}
homicide_df |> 
  group_by(city_state) |> 
  summarize(
    n_hom = n(),
    "Number of Solved Homicides" = sum(disposition == "Closed by arrest", na.rm = TRUE),
    "Number of Unsolved Homicides" = sum(disposition != "Closed by arrest", na.rm = TRUE),
    "% Female Victims" = paste0(round(sum(victim_sex == "Female") / n() * 100, 1), "%"),
    "% Male Victims" = paste0(round(sum(victim_sex == "Male") / n() * 100, 1), "%"),
    "% Unknown Gender Victims" = paste0(round(sum(victim_sex == "Unknown") / n() * 100, 1), "%"),
  ) |> 
  arrange(-n_hom) |>
  rename("Total Number of Homicides" = n_hom) |> 
  head(10) |> 
  knitr::kable()
```

#### **Homicide Count Summary Table**

Here are the numbers of total homicides and unsolved homicides in each city.
```{r summarize_unsolved_homicides}
homicide_df |> 
  group_by(city_state) |> 
  summarize(
    n_homicides = n(),
    n_unsolved = sum(disposition != "Closed by arrest", na.rm = TRUE),
    pct_unsolved = paste0(round(n_unsolved / n_homicides * 100, 1), "%")
  ) |> 
  rename(
    "City, State" = city_state,
    "Number of Homicides" = n_homicides,
    "Number of Unsolved Homicides" = n_unsolved,
    "% Unsolved Homicides" = pct_unsolved
  ) |> 
  knitr::kable()
```

#### **Proportion test for unsolved homicides in Baltimore, MD.**
```{r pr_unsolved_baltimore}
# Create sub-dataframe with only data from Baltimore, MD
baltimore_df = homicide_df |> 
  filter(city_state == "Baltimore, MD")

# Perform proportion test and save results
baltimore_prop_test_result = 
  prop.test(
    x = sum(baltimore_df$disposition != "Closed by arrest", na.rm = TRUE),
    n = nrow(baltimore_df)) |> 
  broom::tidy()

# Pull estimate and confidence interval
baltimore_estimate = as.numeric(baltimore_prop_test_result$estimate) |> round(3)
baltimore_ci_ll = as.numeric(baltimore_prop_test_result$conf.low) |> round(3)
baltimore_ci_ul = as.numeric(baltimore_prop_test_result$conf.high) |> round(3)
```

**Result:** Based on the results of this proportion test, we can see that the estimated proportion is `r baltimore_estimate` with a 95% CI [`r baltimore_ci_ll`, `r baltimore_ci_ul`].

#### **Replicate Process for all Cities**

To do this, first I will write a function to extract the proportion estimate and confidence interval when given the input arguments for the prop.test function.
```{r perform_prop_test_function}
get_prop = function(x, n){
  # Perform proportion test and define key resulting values
  result = prop.test(x, n) |>
    broom::tidy() |> 
    suppressWarnings()
  
  # Generate and return output
  output = tibble(
    estimate = as.numeric(result$estimate) |> round(3),
    ci_ul = as.numeric(result$conf.low) |> round(3),
    ci_ll = as.numeric(result$conf.high) |> round(3)
  )
  return(output)
}
```

Then, I will use the map2 function from the purrr library to apply my new get_prop function to all cities' data.
```{r all_cities_prop_test}
# Run prop.test values for all cities
city_props_df = homicide_df |> 
  group_by(city_state) |> 
  summarize(
    n_unsolved = sum(disposition != "Closed by arrest", na.rm = TRUE),
    n_homicides = n()
  ) |> 
  ungroup() |> 
  mutate(prop_test = map2(n_unsolved, n_homicides, get_prop)) |> 
  unnest(prop_test)

# Preview result
city_props_df |> 
  head(5) |> 
  knitr::kable()
```

#### **Visualizing all Cities' Proportion Test Results**

The following plot visualizes the proportion estimates and confidence intervals for unsolved homicides in all cities in the datasets.

_**Note:** The confidence interval for unsolved homicide proportion in Tulsa, AL is extremely large. This is because Tulsa, AL has only one homicide and zero unsolved homicides recorded in this dataset. Very small sample sizes yield highly uncertain results in the proportion test function, hence the large errorbar._

```{r plot_all_cities_prop_test}
# Reclassify city_state column as factor type, leveled by -(prop estimate)
city_props_df = city_props_df |> 
  mutate(city_state = fct_reorder(city_state, estimate, .na_rm = TRUE)) 

# Generate plot
city_props_df |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_ll, ymax = ci_ul)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = paste0("Unsolved Homicides in ", nrow(city_props_df), " U.S. Cities"),
    x = "City, State",
    y = "Proportion of Homicides\nthat are Unsolved"
  )
```







